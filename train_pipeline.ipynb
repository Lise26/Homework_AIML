{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "The following blocks are to be executed before of anything else, to setup import, classes, functions and \n",
    "constants that are needed for all stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T12:49:06.551457Z",
     "start_time": "2020-05-31T12:49:05.088462Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n",
    "from objectAttentionModelConvLSTM import attentionModel\n",
    "from flow_resnet import flow_resnet34\n",
    "from twoStreamModel import twoStreamAttentionModel\n",
    "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
    "                                RandomHorizontalFlip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T12:49:06.559415Z",
     "start_time": "2020-05-31T12:49:06.553412Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "RGB_PREFIX = 'model_rgb_state_dict'\n",
    "FLOW_PREFIX = 'model_flow_state_dict'\n",
    "JOINT_PREFIX = 'model_twoStream_state_dict'\n",
    "LOG_PREFIX = 'log_stage'\n",
    "VAL_LOG_PREFIX = 'val_log_stage'\n",
    "DATA_DIR = '../GTEA61'\n",
    "model_folder = '../saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T12:49:07.226913Z",
     "start_time": "2020-05-31T12:49:07.216793Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Logger():\n",
    "    def __init__(self, **params):\n",
    "        self.params = params\n",
    "        self.data = []\n",
    "        self.step_data = []\n",
    "        \n",
    "    def add_epoch_data(self, epoch, acc, loss):\n",
    "        self.data.append({epoch:(acc, loss)})\n",
    "        \n",
    "    def add_step_data(self, step, acc, loss):\n",
    "        self.step_data.append({step:(acc, loss)})\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as logfile:\n",
    "            pickle.dump(self, logfile)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, 'rb') as logfile:\n",
    "            new_instance = pickle.load(logfile)\n",
    "        return new_instance\n",
    "\n",
    "    \n",
    "def generate_model_checkpoint_name(stage, n_frames, ms_block=False, loss=None, optional=''):\n",
    "    name = \"\"\n",
    "    if stage < 3:\n",
    "        name += RGB_PREFIX\n",
    "        if stage == 2:\n",
    "            name += '_stage2'\n",
    "    elif stage == 3:\n",
    "        name += FLOW_PREFIX\n",
    "    else:\n",
    "        name += JOINT_PREFIX\n",
    "    name += '_'+str(n_frames)+'frames'\n",
    "    if loss is not None:\n",
    "            name += '_'+loss\n",
    "    if ms_block:\n",
    "        name += '_msblock'\n",
    "    name += optional+\".pth\"\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "def generate_log_filenames(stage, n_frames, ms_block=False, loss=None, optional=''):\n",
    "    train = LOG_PREFIX + str(stage) + '_'+str(n_frames)+'frames'\n",
    "    val = VAL_LOG_PREFIX + str(stage) + '_'+str(n_frames)+'frames'\n",
    "    if loss is not None:\n",
    "        train += '_'+loss\n",
    "        val += '_'+loss\n",
    "    if ms_block:\n",
    "        train += '_msblock'\n",
    "        val  += '_msblock'\n",
    "    train += optional+\".obj\"\n",
    "    val  += optional+\".obj\"\n",
    "    \n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T12:49:08.443058Z",
     "start_time": "2020-05-31T12:49:08.437577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "                             ToTensor(), normalize])\n",
    "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0, that is, Stages 1 and 2 without the cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0.1 specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T19:31:10.649959Z",
     "start_time": "2020-05-30T19:31:10.645355Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 0\n",
    "\n",
    "LR = 0.001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 200      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "# this dictionary is needed for the logger class\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T19:31:12.295651Z",
     "start_time": "2020-05-30T19:31:11.906641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T19:31:13.414416Z",
     "start_time": "2020-05-30T19:31:13.407153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Stage 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T19:31:21.014114Z",
     "start_time": "2020-05-30T19:31:18.741145Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T19:34:40.473744Z",
     "start_time": "2020-05-30T19:31:39.685435Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, optional='_1_NOCAM')\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, optional='_1_NOCAM')\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "train_logger = Logger(**parameters)\n",
    "val_logger = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "        \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable, no_cam=True)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "            \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "    \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            val_logger.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "\n",
    "            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val||||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "        \n",
    "    train_logger.save(train_log_file)\n",
    "    val_logger.save(val_log_file)\n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0.2 specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-31T12:49:39.573408Z",
     "start_time": "2020-05-31T12:49:39.567465Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 0\n",
    "\n",
    "LR = 0.0001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 150      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_old_stage = model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, optional='_1_NOCAM')\n",
    "stage1_dict = os.path.join(model_folder, best_old_stage)\n",
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.load_state_dict(torch.load(stage1_dict))\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "        \n",
    "for params in model.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "#\n",
    "for params in model.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "#\n",
    "for params in model.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "model.resNet.layer4[0].conv1.train(True)\n",
    "model.resNet.layer4[0].conv2.train(True)\n",
    "model.resNet.layer4[1].conv1.train(True)\n",
    "model.resNet.layer4[1].conv2.train(True)\n",
    "model.resNet.layer4[2].conv1.train(True)\n",
    "model.resNet.layer4[2].conv2.train(True)\n",
    "model.resNet.fc.train(True)\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, optional='_2_NOCAM')\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, optional='_2_NOCAM')\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "train_logger_2 = Logger(**parameters)\n",
    "val_logger_2 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    \n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "    model.resNet.layer4[0].conv1.train(True)\n",
    "    model.resNet.layer4[0].conv2.train(True)\n",
    "    model.resNet.layer4[1].conv1.train(True)\n",
    "    model.resNet.layer4[1].conv2.train(True)\n",
    "    model.resNet.layer4[2].conv1.train(True)\n",
    "    model.resNet.layer4[2].conv2.train(True)\n",
    "    model.resNet.fc.train(True)\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_2.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger_2.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    \n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate is not None:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable, no_cam=True)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_2.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "\n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            val_logger_2.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "            \n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val |||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    optim_scheduler.step()\n",
    "    train_logger_2.save(train_log_file)\n",
    "    val_logger_2.save(val_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:12:04.625629Z",
     "start_time": "2020-05-30T18:12:04.615850Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 1\n",
    "\n",
    "LR = 0.001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 200      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "# this dictionary is needed for the logger class\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T15:32:48.885046Z",
     "start_time": "2020-05-30T15:32:48.320636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T15:32:59.333583Z",
     "start_time": "2020-05-30T15:32:59.329306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T15:33:07.680329Z",
     "start_time": "2020-05-30T15:33:03.222399Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T12:02:41.949191Z",
     "start_time": "2020-05-23T12:02:41.946130Z"
    }
   },
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:52:06.421156Z",
     "start_time": "2020-05-29T17:51:42.395153Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 61])\n"
     ]
    }
   ],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN)\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN)\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "train_logger = Logger(**parameters)\n",
    "val_logger = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "        \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "            \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "    \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            val_logger.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "\n",
    "            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val||||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "        \n",
    "    train_logger.save(train_log_file)\n",
    "    val_logger.save(val_log_file)\n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:15:04.564245Z",
     "start_time": "2020-05-30T18:15:04.556527Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 2\n",
    "\n",
    "LR = 0.0001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 150      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_old_stage = generate_model_checkpoint_name(stage=1, n_frames=SEQ_LEN)\n",
    "stage1_dict = os.path.join(model_folder, best_old_stage)\n",
    "validate = True\n",
    "\n",
    "model = attentionModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.load_state_dict(torch.load(stage1_dict))\n",
    "model.train(False)\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "        \n",
    "for params in model.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "model.resNet.layer4[0].conv1.train(True)\n",
    "model.resNet.layer4[0].conv2.train(True)\n",
    "model.resNet.layer4[1].conv1.train(True)\n",
    "model.resNet.layer4[1].conv2.train(True)\n",
    "model.resNet.layer4[2].conv1.train(True)\n",
    "model.resNet.layer4[2].conv2.train(True)\n",
    "model.resNet.fc.train(True)\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN)\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN)\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "train_logger_2 = Logger(**parameters)\n",
    "val_logger_2 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    \n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "    model.resNet.layer4[0].conv1.train(True)\n",
    "    model.resNet.layer4[0].conv2.train(True)\n",
    "    model.resNet.layer4[1].conv1.train(True)\n",
    "    model.resNet.layer4[1].conv2.train(True)\n",
    "    model.resNet.layer4[2].conv1.train(True)\n",
    "    model.resNet.layer4[2].conv2.train(True)\n",
    "    model.resNet.fc.train(True)\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_2.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger_2.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    \n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "    if validate is not None:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_2.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "\n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            val_logger_2.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "            \n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val |||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    optim_scheduler.step()\n",
    "    train_logger_2.save(train_log_file)\n",
    "    val_logger_2.save(val_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal network specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:17:30.991878Z",
     "start_time": "2020-05-30T18:17:30.986520Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 3\n",
    "\n",
    "LR = 0.01            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 750      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [150, 300, 500] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.5          # Multiplicative factor for learning rate step-down\n",
    "STACK_SIZE = 5\n",
    "\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:59:01.228238Z",
     "start_time": "2020-05-28T16:59:00.713373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61_flow(DATA_DIR, split='train', transform=spatial_transform_train, seq_len=STACK_SIZE)\n",
    "test_dataset = GTEA61_flow(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=STACK_SIZE)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:59:03.403407Z",
     "start_time": "2020-05-28T16:59:03.398494Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T16:59:10.486808Z",
     "start_time": "2020-05-28T16:59:06.518557Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = flow_resnet34(True, channels=2*STACK_SIZE, num_classes=NUM_CLASSES)\n",
    "model.train(True)\n",
    "train_params = list(model.parameters())\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = torch.optim.SGD(train_params, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:01.771262Z",
     "start_time": "2020-05-24T15:16:44.278334Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, STACK_SIZE)\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN)\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "\n",
    "train_logger_3 = Logger(**parameters)\n",
    "val_logger_3 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.train(True)\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        inputVariable = inputs.to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        \n",
    "        output_label, _ = model(inputVariable)\n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_3.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_loss, trainAccuracy))\n",
    "    train_logger_3.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "        \n",
    "    if validate:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                val_samples += inputs.size(0)\n",
    "                inputVariable = inputs.to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                step_loss = val_loss.data.item()\n",
    "                val_loss_epoch += step_loss\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_3.add_step_data(val_iter, numCorr, step_loss)\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            val_logger_3.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "            \n",
    "            print('Validation: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    optim_scheduler.step()\n",
    "    train_logger_3.save(train_log_file)\n",
    "    val_logger_3.save(val_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:20:13.394143Z",
     "start_time": "2020-05-24T13:20:13.390030Z"
    }
   },
   "source": [
    "# 2 Stream joint training specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T18:19:10.464167Z",
     "start_time": "2020-05-30T18:19:10.459236Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 4\n",
    "\n",
    "LR = 0.01            # The initial Learning Rate\n",
    "LR_FLOW = 0.0001\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-4  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 250      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 1 # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.99          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "STACK_SIZE = 5\n",
    "SEQ_LEN = 7\n",
    "\n",
    "# this dictionary is needed for the logger class\n",
    "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
    "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
    "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':512, 'SEQ_LEN':SEQ_LEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:23:01.475430Z",
     "start_time": "2020-05-27T17:23:01.203650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61_2Stream(DATA_DIR, split='train', transform=spatial_transform_train, seq_len=SEQ_LEN, \n",
    "                               stack_size=STACK_SIZE)\n",
    "test_dataset = GTEA61_2Stream(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN, \n",
    "                              stack_size=STACK_SIZE)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:23:02.025071Z",
     "start_time": "2020-05-27T17:23:02.021724Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:30:15.034943Z",
     "start_time": "2020-05-27T17:30:09.603289Z"
    }
   },
   "outputs": [],
   "source": [
    "flow_model_name = generate_model_checkpoint_name(stage=3, n_frames=STACK_SIZE)\n",
    "rgb_model_name = generate_model_checkpoint_name(stage=2, n_frames=SEQ_LEN)\n",
    "flowModel = os.path.join(model_folder, flow_model_name)\n",
    "rgbModel = os.path.join(model_folder, rgb_model_name)\n",
    "validate = True\n",
    "\n",
    "model = twoStreamAttentionModel(flowModel=flowModel, frameModel=rgbModel, stackSize=STACK_SIZE, memSize=MEM_SIZE,\n",
    "                                    num_classes=NUM_CLASSES)\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "model.train(False)\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[0].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[0].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "       \n",
    "for params in model.frameModel.resNet.layer4[1].conv1.parameters():\n",
    "    params.requires_grad = True     \n",
    "\n",
    "for params in model.frameModel.resNet.layer4[1].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "       \n",
    "for params in model.frameModel.resNet.layer4[2].conv1.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.frameModel.resNet.layer4[2].conv2.parameters():\n",
    "    params.requires_grad = True\n",
    "            \n",
    "for params in model.frameModel.resNet.fc.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.flowModel.layer4.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "\n",
    "model.classifier.train(True)\n",
    "model.flowModel.layer4.train(True)\n",
    "model.frameModel.lstm_cell.train(True)\n",
    "model.frameModel.classifier.train(True)\n",
    "\n",
    "frame_trainable_params = [p for p in model.frameModel.parameters() if p.requires_grad]\n",
    "flow_trainable_params = [p for p in model.flowModel.parameters() if p.requires_grad]\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "optimizer_fn = torch.optim.SGD([\n",
    "        {'params': frame_trainable_params},\n",
    "        {'params': flow_trainable_params, 'lr': LR_FLOW},\n",
    "    ], lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.StepLR(optimizer_fn, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T17:36:26.054236Z",
     "start_time": "2020-05-27T17:32:48.646035Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, loss='NLL')\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN)\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "train_logger_4 = Logger(**parameters)\n",
    "val_logger_4 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    model.classifier.train(True)\n",
    "    model.flowModel.layer4.train(True)\n",
    "    model.frameModel.lstm_cell.train(True)\n",
    "    model.frameModel.classifier.train(True)\n",
    "    for j, (inputFlow, inputFrame, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "        optimizer_fn.zero_grad()\n",
    "        \n",
    "        inputVariableFlow = inputFlow.to(DEVICE)\n",
    "        inputVariableFrame = inputFrame.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        \n",
    "        output_label = model(inputVariableFlow, inputVariableFrame)\n",
    "        loss = loss_fn(F.log_softmax(output_label, dim=1), labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_4.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "\n",
    "    avg_loss = epoch_loss / iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger_4.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "\n",
    "    print('Average training loss after {} epoch = {:.3f} '.format(epoch + 1, avg_loss))\n",
    "    print('Training accuracy after {} epoch = {:.3f}% '.format(epoch + 1, trainAccuracy))\n",
    "    \n",
    "    if validate:\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputFlow, inputFrame, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "                \n",
    "                inputVariableFlow = inputFlow.to(DEVICE)\n",
    "                inputVariableFrame = inputFrame.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                 \n",
    "                output_label = model(inputVariableFlow, inputVariableFrame)\n",
    "                val_loss = loss_fn(F.log_softmax(output_label, dim=1), labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_4.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "\n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_iter\n",
    "            val_logger_4.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "\n",
    "            print('Val Loss after {} epochs, loss = {:.3f}'.format(epoch + 1, avg_val_loss))\n",
    "            print('Val Accuracy after {} epochs = {:.3f}%'.format(epoch + 1, val_accuracy))\n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val |||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    train_logger_4.save(train_log_file)\n",
    "    val_logger_4.save(val_log_file)\n",
    "    optim_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
