{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stage2_tuning_pipeline.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO9+KN0jEXphn/LqFeoIIYu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cPAQpsvWynjD","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfiRjXH3yQ6A","colab_type":"code","colab":{}},"source":["%cd drive/My\\ Drive/\n","!git clone https://[username]:[password]@github.com/Erosinho13/FPAR-Project-MLDL.git\n","%cd FPAR-Project-MLDL/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v2lsdTrTBZrA","colab_type":"text"},"source":["# SETUP"]},{"cell_type":"code","metadata":{"id":"LYvo2IURiC_m","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.backends import cudnn\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.models import resnet34\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from logs import Logger, generate_model_checkpoint_name, generate_log_filenames\n","from gridsearch import GridSearch\n","\n","from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n","from AttentionModelMS import attention_model_ms\n","from flow_resnet import flow_resnet34\n","from twoStreamModel import twoStreamAttentionModel\n","from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n","                                RandomHorizontalFlip, DownSampling, To1Dimension)\n","\n","def runConstants(device = 'cuda', num_classes = 61, batch_size = 32, lr = 0.0001, \\\n","                 momentum = 0.9, weight_decay = 4e-5, num_epochs = 60, step_size = [50], \\\n","                 gamma = 0.1, mem_size = 512, seq_len = 16):\n","    \n","    global DEVICE\n","    global NUM_CLASSES\n","    global BATCH_SIZE\n","    global LR\n","    global MOMENTUM\n","    global WEIGHT_DECAY\n","    global NUM_EPOCHS\n","    global STEP_SIZE\n","    global GAMMA\n","    global MEM_SIZE\n","    global SEQ_LEN\n","    \n","    DEVICE = device\n","    NUM_CLASSES = num_classes\n","    BATCH_SIZE = batch_size\n","    LR = lr\n","    MOMENTUM = momentum\n","    WEIGHT_DECAY = weight_decay\n","    NUM_EPOCHS = num_epochs\n","    STEP_SIZE = step_size\n","    GAMMA = gamma\n","    MEM_SIZE = mem_size\n","    SEQ_LEN = seq_len\n","\n","STAGE = 2\n","MMAP_LENGTH = 49\n","DATA_DIR = '../GTEA61'\n","model_folder = '../saved_models'\n","BEST_MODEL_OPT = '_step_run1'\n","OPTIONAL = '_tuning_run'\n","OFFSET = 0\n","\n","# Data loader\n","normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","default_transforms = [\n","    Scale(256),\n","    RandomHorizontalFlip(),\n","    MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n","    ToTensor()\n","]\n","\n","spatial_transform = Compose(default_transforms + [normalize])\n","spatial_transform_mmaps = Compose(default_transforms + [DownSampling(), To1Dimension()])\n","spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aCC-ToO7hI4B","colab_type":"text"},"source":["# GRIDSEARCH SETUP"]},{"cell_type":"code","metadata":{"id":"JtEkpWN0hIOD","colab_type":"code","colab":{}},"source":["params = {\n","    'device': ['cuda'],\n","    'num_classes': [61],\n","    'batch_size': [32],\n","    'lr': [0.0001, 0.005, 0.001],\n","    'momentum': [0.9],\n","    'weight_decay': [4e-3, 4e-5, 4e-6],\n","    'num_epochs': [100],\n","    'step_size': [[30, 70], [40, 80], [50, 100]],\n","    'gamma': [0.1, 0.2, 0.5],\n","    'mem_size': [512],\n","    'seq_len': [16]\n","}\n","\n","gs = GridSearch(params)\n","\n","with open(\"param_list.txt\", \"w\") as f:\n","    for i in gs.param_list:\n","        f.write(str(i))\n","        f.write('\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sh_sAYoKngoz","colab_type":"text"},"source":["# TUNING"]},{"cell_type":"code","metadata":{"id":"3FIsSrKsnfYQ","colab_type":"code","colab":{}},"source":["best_accuracy = 0\n","\n","for i, param_set in enumerate(gs.param_list):\n","\n","    print()\n","    print()\n","    print(f\"#############################################################################\")\n","    print(f\"################################## RUN {i} ##################################\")\n","    print(f\"#############################################################################\")\n","    print()\n","    print(f\"PARAMETERS: {param_set}\")\n","    print()\n","\n","    runConstants(**param_set)\n","\n","    # Prepare Pytorch train/test Datasets\n","    train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform,\n","                        seq_len=SEQ_LEN, mmaps = True, mmaps_transform = spatial_transform_mmaps)\n","    test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val,\n","                        seq_len=SEQ_LEN)\n","\n","    # Check dataset sizes\n","    print('Train Dataset: {}'.format(len(train_dataset)))\n","    print('Test Dataset: {}'.format(len(test_dataset)))\n","\n","    # Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","    val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","\n","    best_old_stage = generate_model_checkpoint_name(stage = 1, n_frames = 16, ms_block = True, \\\n","                                                    optional = BEST_MODEL_OPT)\n","    stage1_dict = os.path.join(model_folder, best_old_stage)\n","    validate = True\n","\n","    model = attention_model_ms(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n","    model.load_state_dict(torch.load(stage1_dict))\n","\n","    model.train(False)\n","\n","    for params in model.parameters():\n","        params.requires_grad = False\n","\n","    layers_to_train = [\n","        model.resNet.layer4[0].conv1,\n","        model.resNet.layer4[0].conv2,\n","        model.resNet.layer4[1].conv1,\n","        model.resNet.layer4[1].conv2,\n","        model.resNet.layer4[2].conv1,\n","        model.resNet.layer4[2].conv2,\n","        model.resNet.fc,\n","        model.lstm_cell,\n","        model.classifier,\n","        model.msBlock\n","    ]\n","\n","    for layer in layers_to_train:\n","        for params in layer.parameters():\n","            params.requires_grad = True\n","\n","    for layer in layers_to_train:\n","        layer.train(True)\n","\n","    model.to(DEVICE)\n","\n","    loss_fn = nn.CrossEntropyLoss()\n","    loss_fn_sum = nn.CrossEntropyLoss(reduction = 'sum')\n","\n","    trainable_params = [p for p in model.parameters() if p.requires_grad]\n","    optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n","\n","    optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n","\n","    train_iter = 0\n","    val_iter = 0\n","    min_accuracy = 0\n","\n","    trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n","    val_samples = len(test_dataset)\n","\n","    iterPerEpoch = len(train_loader)\n","    val_steps = len(val_loader)\n","\n","    cudnn.benchmark\n","    torch.autograd.set_detect_anomaly(True)\n","\n","    current_run = OFFSET + i\n","    str_opt = OPTIONAL + \"{:02d}\".format(current_run)\n","\n","    train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, ms_block = True, \\\n","                                                optional = str_opt)\n","    model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, ms_block = True, \\\n","                                                    optional = str_opt)\n","\n","    train_log_file = os.path.join(model_folder, train_log)\n","    val_log_file = os.path.join(model_folder, val_log)\n","    train_logger_2 = Logger(**param_set)\n","    val_logger_2 = Logger(**param_set)\n","\n","    for epoch in range(NUM_EPOCHS):\n","\n","        label_epoch_loss = 0\n","        mmaps_epoch_loss = 0\n","        numCorrTrain = 0\n","        \n","        for layer in layers_to_train:\n","            layer.train(True)\n","        \n","        for i, (inputs, mmaps, targets) in enumerate(train_loader):\n","\n","            mmaps = mmaps.permute(1, 0, 2)\n","\n","            train_iter += 1\n","\n","            optimizer_fn.zero_grad()\n","\n","            inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","            labelVariable = targets.to(DEVICE)\n","\n","            output_label, _, output_label_mmaps = model(inputVariable, mmaps = True)\n","\n","            output_label_mmaps = torch.flatten(output_label_mmaps).unsqueeze_(-1) + torch.zeros(2).to(DEVICE)\n","            output_label_mmaps[:, 0] = 1 - output_label_mmaps[:, 1]\n","            output_label_mmaps = output_label_mmaps.to(DEVICE)\n","            \n","            mmaps = torch.flatten(mmaps).long().to(DEVICE)\n","\n","            mmaps_loss = loss_fn_sum(output_label_mmaps, mmaps)/(BATCH_SIZE * SEQ_LEN)\n","            label_loss = loss_fn(output_label, labelVariable)\n","            loss = mmaps_loss + label_loss\n","\n","            loss.backward()\n","\n","            optimizer_fn.step()\n","            \n","            _, predicted = torch.max(output_label.data, 1)\n","            numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n","            \n","            label_step_loss = label_loss.data.item()\n","            label_epoch_loss += label_step_loss\n","            \n","            mmaps_step_loss = mmaps_loss.data.item()\n","            mmaps_epoch_loss += mmaps_step_loss\n","\n","            # train_logger_2.add_step_data(train_iter, numCorrTrain, step_loss)\n","            \n","        label_avg_loss = label_epoch_loss/iterPerEpoch\n","        mmaps_avg_loss = mmaps_epoch_loss/iterPerEpoch\n","        trainAccuracy = (numCorrTrain / trainSamples) * 100\n","        train_logger_2.add_epoch_data(epoch+1, trainAccuracy, label_avg_loss, mmaps_avg_loss)\n","        \n","        print('Train: Epoch = {} | label_avg_loss = {:.3f} | mmaps_avg_loss = {:.3f} | Accuracy = {:.3f}'.format(\n","            epoch+1, label_avg_loss, mmaps_avg_loss, trainAccuracy))\n","\n","        if validate is not None:\n","\n","            if (epoch+1) % 1 == 0:\n","\n","                model.train(False)\n","                val_loss_epoch = 0\n","                numCorr = 0\n","\n","                for j, (inputs, targets) in enumerate(val_loader):\n","\n","                    val_iter += 1\n","\n","                    inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n","                    labelVariable = targets.to(DEVICE)\n","                    \n","                    output_label, _ = model(inputVariable)\n","\n","                    val_loss = loss_fn(output_label, labelVariable)\n","                    val_loss_step = val_loss.data.item()\n","                    val_loss_epoch += val_loss_step\n","\n","                    _, predicted = torch.max(output_label.data, 1)\n","\n","                    numCorr += torch.sum(predicted == labelVariable.data).data.item()\n","                    val_logger_2.add_step_data(val_iter, numCorr, val_loss_step)\n","\n","                val_accuracy = (numCorr / val_samples) * 100\n","                avg_val_loss = val_loss_epoch / val_steps\n","\n","                print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n","                \n","                val_logger_2.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n","                \n","                if val_accuracy > min_accuracy:\n","                    print(\"[||| NEW BEST on val |||]\")\n","                    save_path_model = os.path.join(model_folder, model_checkpoint)\n","                    torch.save(model.state_dict(), save_path_model)\n","                    min_accuracy = val_accuracy\n","                \n","        optim_scheduler.step()\n","\n","        train_logger_2.save(train_log_file)\n","        val_logger_2.save(val_log_file)\n","\n","    if min_accuracy > best_accuracy:\n","        print(\"********************* NEW BEST SET OF VALUES FOR THE HYPERPARAMETERS FOUND *********************\")\n","        print(param_set)\n","\n","        best_set = param_set\n","        best_accuracy = min_accuracy\n","\n","print()\n","print(\"§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§\")\n","print(\"§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§\")\n","print(\"§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§\")\n","print()\n","print(\"BEST SET OF VALUES FOR THE HYPERPARAMETERS FOUND:\")\n","print(best_set)\n","print()"],"execution_count":0,"outputs":[]}]}
