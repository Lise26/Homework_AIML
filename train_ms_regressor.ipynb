{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP\n",
    "The following blocks are to be executed first of anything else, to setup import, classes, functions and \n",
    "constants that are needed for all stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T16:53:19.857817Z",
     "start_time": "2020-06-04T16:53:17.724795Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from logs import Logger, generate_model_checkpoint_name, generate_log_filenames\n",
    "\n",
    "from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n",
    "from attention_model_MS_reg import attention_model_ms_reg\n",
    "from flow_resnet import flow_resnet34\n",
    "from twoStreamModel import twoStreamAttentionModel\n",
    "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
    "                                RandomHorizontalFlip, DownSampling, To1Dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T16:53:20.023973Z",
     "start_time": "2020-06-04T16:53:20.018928Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "NUM_CLASSES = 61 # 101 + 1: There is am extra Background class that should be removed \n",
    "\n",
    "BATCH_SIZE = 32     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "MMAP_LENGTH = 49\n",
    "DATA_DIR = '../GTEA61'\n",
    "model_folder = '../saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T16:53:26.617978Z",
     "start_time": "2020-06-04T16:53:26.613369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "default_transforms = [\n",
    "    Scale(256),\n",
    "    RandomHorizontalFlip(),\n",
    "    MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
    "    ToTensor()\n",
    "]\n",
    "\n",
    "spatial_transform = Compose(default_transforms + [normalize])\n",
    "spatial_transform_mmaps = Compose(default_transforms + [DownSampling(), To1Dimension()])\n",
    "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 specific-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T13:26:55.442526Z",
     "start_time": "2020-06-03T13:26:55.436464Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 1\n",
    "\n",
    "LR = 0.001                            # The initial Learning Rate\n",
    "MOMENTUM = 0.9                        # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5                   # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 200                      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75, 150]             # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1                           # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "# this dictionary is needed for the logger class\n",
    "parameters = {\n",
    "    'DEVICE': DEVICE,\n",
    "    'NUM_CLASSES': NUM_CLASSES,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': LR,\n",
    "    'MOMENTUM': MOMENTUM,\n",
    "    'WEIGHT_DECAY': WEIGHT_DECAY,\n",
    "    'NUM_EPOCHS': NUM_EPOCHS,\n",
    "    'STEP_SIZE': STEP_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'MEM_SIZE': MEM_SIZE,\n",
    "    'SEQ_LEN': SEQ_LEN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T13:26:56.412014Z",
     "start_time": "2020-06-03T13:26:56.346966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split = 'train', transform = spatial_transform, seq_len = SEQ_LEN,\n",
    "                       mmaps = True, mmaps_transform = spatial_transform_mmaps)\n",
    "test_dataset = GTEA61(DATA_DIR, split = 'test', transform = spatial_transform_val, seq_len = SEQ_LEN)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T13:26:57.357988Z",
     "start_time": "2020-06-03T13:26:57.352892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4, drop_last = True)\n",
    "val_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:41:56.667997Z",
     "start_time": "2020-06-03T14:41:55.679995Z"
    }
   },
   "outputs": [],
   "source": [
    "validate = True\n",
    "\n",
    "model = attention_model_ms_reg(num_classes = NUM_CLASSES, mem_size = MEM_SIZE)\n",
    "model.train(False)\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "for params in model.lstm_cell.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "for params in model.classifier.parameters():\n",
    "    params.requires_grad = True\n",
    "\n",
    "model.lstm_cell.train(True)\n",
    "model.classifier.train(True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_fn = optim.Adam(trainable_params, lr = LR, weight_decay = WEIGHT_DECAY, eps = 1e-4)\n",
    "\n",
    "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones = STEP_SIZE, gamma = GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset) \n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, ms_block = True)\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, ms_block = True)\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "\n",
    "train_logger = Logger(**parameters)\n",
    "val_logger = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    \n",
    "    model.lstm_cell.train(True)\n",
    "    model.classifier.train(True)\n",
    "        \n",
    "    for i, (inputs, _, targets) in enumerate(train_loader):\n",
    "\n",
    "        train_iter += 1\n",
    "\n",
    "        optimizer_fn.zero_grad()\n",
    "\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        output_label, _ = model(inputVariable)\n",
    "        \n",
    "        loss = loss_fn(output_label, labelVariable)\n",
    "        loss.backward()\n",
    "        optimizer_fn.step()\n",
    "            \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "\n",
    "        train_logger.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "    \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "\n",
    "    if validate:\n",
    "\n",
    "        if (epoch+1) % 1 == 0:\n",
    "\n",
    "            model.train(False)\n",
    "\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "\n",
    "                val_iter += 1\n",
    "\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "                \n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "\n",
    "                val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "                \n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "            val_logger.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "\n",
    "            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "\n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val||||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "        \n",
    "    train_logger.save(train_log_file)\n",
    "    val_logger.save(val_log_file)\n",
    "    optim_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 specific setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T17:15:53.282632Z",
     "start_time": "2020-06-04T17:15:53.276427Z"
    }
   },
   "outputs": [],
   "source": [
    "STAGE = 2\n",
    "\n",
    "LR = 0.0001            # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 150      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = [25, 75] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "MEM_SIZE = 512\n",
    "SEQ_LEN = 7\n",
    "\n",
    "parameters = {\n",
    "    'DEVICE': DEVICE,\n",
    "    'NUM_CLASSES': NUM_CLASSES,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': LR,\n",
    "    'MOMENTUM': MOMENTUM,\n",
    "    'WEIGHT_DECAY': WEIGHT_DECAY,\n",
    "    'NUM_EPOCHS': NUM_EPOCHS,\n",
    "    'STEP_SIZE': STEP_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'MEM_SIZE': MEM_SIZE,\n",
    "    'SEQ_LEN': SEQ_LEN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T17:15:54.282018Z",
     "start_time": "2020-06-04T17:15:54.253289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 341\n",
      "Test Dataset: 116\n"
     ]
    }
   ],
   "source": [
    "# Prepare Pytorch train/test Datasets\n",
    "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform,\n",
    "                       seq_len=SEQ_LEN, mmaps = True, mmaps_transform = spatial_transform_mmaps)\n",
    "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val,\n",
    "                      seq_len=SEQ_LEN)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T17:15:55.714859Z",
     "start_time": "2020-06-04T17:15:55.711420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T17:42:34.499819Z",
     "start_time": "2020-06-04T17:42:33.191865Z"
    }
   },
   "outputs": [],
   "source": [
    "best_old_stage = generate_model_checkpoint_name(stage=1, n_frames=SEQ_LEN, ms_block = True)\n",
    "stage1_dict = os.path.join(model_folder, best_old_stage)\n",
    "validate = True\n",
    "\n",
    "model = attention_model_ms_reg(num_classes=NUM_CLASSES, mem_size=MEM_SIZE)\n",
    "model.load_state_dict(torch.load(stage1_dict))\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "layers_to_train = [\n",
    "    model.resNet.layer4[0].conv1,\n",
    "    model.resNet.layer4[0].conv2,\n",
    "    model.resNet.layer4[1].conv1,\n",
    "    model.resNet.layer4[1].conv2,\n",
    "    model.resNet.layer4[2].conv1,\n",
    "    model.resNet.layer4[2].conv2,\n",
    "    model.resNet.fc,\n",
    "    model.lstm_cell,\n",
    "    model.classifier,\n",
    "    model.msBlock\n",
    "]\n",
    "\n",
    "for layer in layers_to_train:\n",
    "    for params in layer.parameters():\n",
    "        params.requires_grad = True\n",
    "\n",
    "for layer in layers_to_train:\n",
    "    layer.train(True)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "loss_class = nn.CrossEntropyLoss()\n",
    "loss_reg = nn.MSELoss(reduction='sum')\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer_fn = torch.optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
    "\n",
    "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T18:09:10.560609Z",
     "start_time": "2020-06-04T17:42:38.071647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Epoch = 1 | Loss = 9.521 | Accuracy = 2.812\n",
      "Val: Epoch = 1 | Loss 4.372 | Accuracy = 1.724\n",
      "[||| NEW BEST on val |||]\n",
      "Train: Epoch = 2 | Loss = 6.913 | Accuracy = 1.875\n",
      "Val: Epoch = 2 | Loss 4.188 | Accuracy = 1.724\n",
      "Train: Epoch = 3 | Loss = 6.577 | Accuracy = 2.500\n",
      "Val: Epoch = 3 | Loss 4.130 | Accuracy = 0.862\n",
      "Train: Epoch = 4 | Loss = 6.409 | Accuracy = 3.750\n",
      "Val: Epoch = 4 | Loss 4.041 | Accuracy = 3.448\n",
      "[||| NEW BEST on val |||]\n",
      "Train: Epoch = 5 | Loss = 6.502 | Accuracy = 4.688\n",
      "Val: Epoch = 5 | Loss 4.020 | Accuracy = 5.172\n",
      "[||| NEW BEST on val |||]\n",
      "Train: Epoch = 6 | Loss = 6.404 | Accuracy = 4.688\n",
      "Val: Epoch = 6 | Loss 3.976 | Accuracy = 6.034\n",
      "[||| NEW BEST on val |||]\n",
      "Train: Epoch = 7 | Loss = 6.294 | Accuracy = 8.125\n",
      "Val: Epoch = 7 | Loss 3.966 | Accuracy = 6.034\n",
      "Train: Epoch = 8 | Loss = 6.447 | Accuracy = 7.812\n",
      "Val: Epoch = 8 | Loss 3.921 | Accuracy = 6.897\n",
      "[||| NEW BEST on val |||]\n",
      "Train: Epoch = 9 | Loss = 6.279 | Accuracy = 6.562\n",
      "Val: Epoch = 9 | Loss 3.907 | Accuracy = 6.897\n",
      "Train: Epoch = 10 | Loss = 6.192 | Accuracy = 5.000\n",
      "Val: Epoch = 10 | Loss 3.907 | Accuracy = 5.172\n",
      "Train: Epoch = 11 | Loss = 6.158 | Accuracy = 8.438\n",
      "Val: Epoch = 11 | Loss 3.856 | Accuracy = 6.897\n",
      "Train: Epoch = 12 | Loss = 5.957 | Accuracy = 8.125\n",
      "Val: Epoch = 12 | Loss 3.914 | Accuracy = 6.897\n",
      "Train: Epoch = 13 | Loss = 6.080 | Accuracy = 9.375\n",
      "Val: Epoch = 13 | Loss 3.835 | Accuracy = 9.483\n",
      "[||| NEW BEST on val |||]\n",
      "Train: Epoch = 14 | Loss = 6.116 | Accuracy = 7.812\n",
      "Val: Epoch = 14 | Loss 3.688 | Accuracy = 11.207\n",
      "[||| NEW BEST on val |||]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-afcde57034cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_label_mmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmi/anaconda3/envs/ml_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scaricati/program/ML_DL/FPAR/FPAR_repo/attention_model_MS_reg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputVariable, no_cam, mmaps)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mfeature_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter = 0\n",
    "val_iter = 0\n",
    "min_accuracy = 0\n",
    "\n",
    "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
    "val_samples = len(test_dataset)\n",
    "\n",
    "iterPerEpoch = len(train_loader)\n",
    "val_steps = len(val_loader)\n",
    "\n",
    "cudnn.benchmark\n",
    "\n",
    "train_log, val_log = generate_log_filenames(STAGE, SEQ_LEN, mmaps=True, optional=\"_reg\")\n",
    "model_checkpoint = generate_model_checkpoint_name(STAGE, SEQ_LEN, mmaps=True, optional=\"_reg\"))\n",
    "\n",
    "train_log_file = os.path.join(model_folder, train_log)\n",
    "val_log_file = os.path.join(model_folder, val_log)\n",
    "train_logger_2 = Logger(**parameters)\n",
    "val_logger_2 = Logger(**parameters)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    numCorrTrain = 0\n",
    "    \n",
    "    for layer in layers_to_train:\n",
    "        layer.train(True)\n",
    "    \n",
    "    for i, (inputs, mmaps, targets) in enumerate(train_loader):\n",
    "        train_iter += 1\n",
    "\n",
    "        optimizer_fn.zero_grad()\n",
    "\n",
    "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "        labelVariable = targets.to(DEVICE)\n",
    "        mmaps = mmaps.permute(1, 0, 2).to(DEVICE)\n",
    "\n",
    "        output_label, _, output_label_mmaps = model(inputVariable, mmaps = True)\n",
    "        \n",
    "        loss_rgb = loss_class(output_label, labelVariable)\n",
    "        loss_mmaps = loss_reg(output_label_mmaps, mmaps)/(SEQ_LEN*BATCH_SIZE)\n",
    "    \n",
    "        loss = loss_rgb + loss_mmaps\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_fn.step()\n",
    "        \n",
    "        _, predicted = torch.max(output_label.data, 1)\n",
    "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
    "        step_loss = loss.data.item()\n",
    "        epoch_loss += step_loss\n",
    "        train_logger_2.add_step_data(train_iter, numCorrTrain, step_loss)\n",
    "\n",
    "        \n",
    "    avg_loss = epoch_loss/iterPerEpoch\n",
    "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
    "    train_logger_2.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
    "    \n",
    "    print('Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
    "\n",
    "    if validate is not None:\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            model.train(False)\n",
    "            val_loss_epoch = 0\n",
    "            numCorr = 0\n",
    "            for j, (inputs, targets) in enumerate(val_loader):\n",
    "                val_iter += 1\n",
    "\n",
    "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
    "                labelVariable = targets.to(DEVICE)\n",
    "                \n",
    "                output_label, _ = model(inputVariable)\n",
    "\n",
    "                val_loss = loss_fn(output_label, labelVariable)\n",
    "                val_loss_step = val_loss.data.item()\n",
    "                val_loss_epoch += val_loss_step\n",
    "\n",
    "                _, predicted = torch.max(output_label.data, 1)\n",
    "\n",
    "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
    "                val_logger_2.add_step_data(val_iter, numCorr, val_loss_step)\n",
    "\n",
    "            val_accuracy = (numCorr / val_samples) * 100\n",
    "            avg_val_loss = val_loss_epoch / val_steps\n",
    "\n",
    "            print('Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "            \n",
    "            val_logger_2.add_epoch_data(epoch+1, val_accuracy, avg_val_loss)\n",
    "            \n",
    "            if val_accuracy > min_accuracy:\n",
    "                print(\"[||| NEW BEST on val |||]\")\n",
    "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
    "                torch.save(model.state_dict(), save_path_model)\n",
    "                min_accuracy = val_accuracy\n",
    "            \n",
    "    optim_scheduler.step()\n",
    "\n",
    "    train_logger_2.save(train_log_file)\n",
    "    val_logger_2.save(val_log_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
